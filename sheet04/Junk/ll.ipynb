{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "# Define the CNN model\n",
    "def create_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(8, activation='relu'),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(4, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "Q_theta = create_model()\n",
    "# Build the model to initialize the weights\n",
    "Q_theta.build(input_shape=(None, 8))\n",
    "\n",
    "# Copy the model\n",
    "Q_target = tf.keras.models.clone_model(Q_theta)\n",
    "Q_target.build(input_shape=(None, 8))\n",
    "Q_target.set_weights(Q_theta.get_weights())\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, number_environments, epsilon=0.2):\n",
    "        self.num_envs = number_environments\n",
    "        self.envs = gym.vector.make('LunarLander-v2',\n",
    "                                    num_envs=number_environments)#,\n",
    "                                    #render_mode='human')\n",
    "        self.current_states, _ = self.envs.reset()\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "\n",
    "    def sample(self, model):\n",
    "        # gather q values for each action for each enironment\n",
    "        q_values = model(self.current_states)\n",
    "        # get actions with highest q values\n",
    "        actions = np.argmax(q_values, axis=1)\n",
    "        # replace chosen action with random action with probability of epsilon\n",
    "        actions = [np.random.choice(4) if np.random.rand() < self.epsilon else action for action in actions]\n",
    "        # take actions\n",
    "        new_states, rewards, _, _, _ = self.envs.step(actions)\n",
    "        old_states = self.current_states\n",
    "        self.current_states = new_states\n",
    "        return old_states, actions, rewards, new_states\n",
    "    \n",
    "\n",
    "\n",
    "class Buffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = []\n",
    "    \n",
    "\n",
    "    def add_to_buffer(self, samples):\n",
    "        for i in range(len(samples[0])):\n",
    "            state = samples[0][i]\n",
    "            action = samples[1][i]\n",
    "            reward = samples[2][i]\n",
    "            new_state = samples[3][i]\n",
    "            sample = np.array([state, action, reward, new_state])\n",
    "            self.buffer.append(sample)\n",
    "        if len(self.buffer) > self.buffer_size:\n",
    "            self.buffer = self.buffer[-self.buffer_size:]\n",
    "    \n",
    "\n",
    "    def sample_minibatch(self, batch_size):\n",
    "        batch_size = min(batch_size, len(self.buffer))\n",
    "        minibatch = random.sample(self.buffer, batch_size)\n",
    "        return np.array(minibatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mt/0n8phlt50rn7h35xhgh97mw80000gn/T/ipykernel_75429/3214720083.py:18: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  Q_target.set_weights((1 - TAU) * np.array(Q_target.get_weights()) + TAU * np.array(Q_theta.get_weights()))\n",
      "/var/folders/mt/0n8phlt50rn7h35xhgh97mw80000gn/T/ipykernel_75429/1425192220.py:38: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sample = np.array([state, action, reward, new_state])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss mean: -6.329098643703439\n",
      "Loss mean: -7.575785463616369\n",
      "Loss mean: -7.928300613603281\n",
      "Loss mean: -7.593817908272199\n",
      "Loss mean: -7.677198870570603\n",
      "Loss mean: -7.580166290616096\n",
      "Loss mean: -7.708190700997901\n",
      "Loss mean: -7.695563462196051\n",
      "Loss mean: -7.569457535967257\n",
      "Loss mean: -7.7296314816354155\n",
      "Loss mean: -7.325621689254945\n",
      "Loss mean: -7.799270940330714\n",
      "Loss mean: -7.439755005908776\n",
      "Loss mean: -7.521089986975595\n",
      "Loss mean: -7.4447152015850016\n",
      "Loss mean: -7.509552935724511\n",
      "Loss mean: -7.383788872896293\n",
      "Loss mean: -7.672284785566479\n",
      "Loss mean: -7.486617077298388\n",
      "Loss mean: -7.399054831656207\n",
      "Loss mean: -7.545158505390708\n",
      "Loss mean: -7.395750569494156\n",
      "Loss mean: -7.503471166250043\n",
      "Loss mean: -7.543768968711386\n",
      "Loss mean: -7.581406252939707\n",
      "Loss mean: -7.346177975682836\n",
      "Loss mean: -7.654995434712149\n",
      "Loss mean: -7.5067973609533905\n",
      "Loss mean: -7.638902488737539\n",
      "Loss mean: -7.5687663967380026\n",
      "Loss mean: -7.751174465209169\n",
      "Loss mean: -7.666591479338564\n",
      "Loss mean: -7.361052956290852\n",
      "Loss mean: -7.487905800618967\n",
      "Loss mean: -7.787843212782797\n",
      "Loss mean: -7.60976698446491\n",
      "Loss mean: -7.44015153764978\n",
      "Loss mean: -7.396178050763063\n",
      "Loss mean: -7.496600501159165\n",
      "Loss mean: -7.333953663911516\n",
      "Loss mean: -7.396655054061222\n",
      "Loss mean: -7.478226497153283\n",
      "Loss mean: -7.46831595986558\n",
      "Loss mean: -7.466766227364276\n",
      "Loss mean: -7.526042151532831\n",
      "Loss mean: -7.410274417463658\n",
      "Loss mean: -7.6520635255910365\n",
      "Loss mean: -7.248108834068374\n",
      "Loss mean: -7.6824043734794785\n",
      "Loss mean: -7.27364988625543\n",
      "Loss mean: -7.7344125790896285\n",
      "Loss mean: -7.591757507050476\n",
      "Loss mean: -7.655671559303986\n",
      "Loss mean: -7.540927261244211\n",
      "Loss mean: -7.453309403369492\n",
      "Loss mean: -7.563732419757813\n",
      "Loss mean: -7.534568230900249\n",
      "Loss mean: -7.486368099281237\n",
      "Loss mean: -7.376503393646379\n",
      "Loss mean: -7.707271668827885\n",
      "Loss mean: -7.186513277017873\n",
      "Loss mean: -7.556888221369825\n",
      "Loss mean: -7.48494324169219\n",
      "Loss mean: -7.645992858714951\n",
      "Loss mean: -7.312587553213419\n",
      "Loss mean: -7.548468463241959\n",
      "Loss mean: -7.272202111466358\n",
      "Loss mean: -7.22372743679761\n",
      "Loss mean: -7.547354561360085\n",
      "Loss mean: -7.659166840161066\n",
      "Loss mean: -7.533913166263301\n",
      "Loss mean: -7.379464286746292\n",
      "Loss mean: -7.334258317103258\n",
      "Loss mean: -7.334155153172217\n",
      "Loss mean: -7.465855975804414\n",
      "Loss mean: -7.382723216030719\n",
      "Loss mean: -7.584788654480509\n",
      "Loss mean: -7.4148568957168655\n",
      "Loss mean: -7.398646699723959\n",
      "Loss mean: -7.655025200402783\n",
      "Loss mean: -7.33256967181386\n",
      "Loss mean: -7.176459022547212\n",
      "Loss mean: -7.244188707011358\n",
      "Loss mean: -7.293287499241117\n",
      "Loss mean: -7.2270168339088805\n",
      "Loss mean: -7.489422752487493\n",
      "Loss mean: -7.433405094767035\n",
      "Loss mean: -7.407355115210873\n",
      "Loss mean: -7.411714087447397\n",
      "Loss mean: -7.3864672285792485\n",
      "Loss mean: -7.36588702364651\n",
      "Loss mean: -7.393911637708351\n",
      "Loss mean: -7.356226429457294\n",
      "Loss mean: -7.418164892907283\n",
      "Loss mean: -7.493088571994082\n",
      "Loss mean: -7.288248162842816\n",
      "Loss mean: -7.350313799100203\n",
      "Loss mean: -7.433541429724506\n",
      "Loss mean: -7.361802322247832\n",
      "Loss mean: -7.417450718747602\n",
      "Loss mean: -7.368883027122219\n",
      "Loss mean: -7.5210292952538245\n",
      "Loss mean: -7.47037739390914\n",
      "Loss mean: -7.500845293629397\n",
      "Loss mean: -7.459199738799673\n",
      "Loss mean: -7.298817011223595\n",
      "Loss mean: -7.517919330608531\n",
      "Loss mean: -7.379373271050328\n",
      "Loss mean: -7.337099228698551\n",
      "Loss mean: -7.294430125182593\n",
      "Loss mean: -7.351802600983373\n",
      "Loss mean: -7.432182132226249\n",
      "Loss mean: -7.328426289578156\n",
      "Loss mean: -7.339507325606227\n",
      "Loss mean: -7.337978686018044\n",
      "Loss mean: -7.702486251376225\n",
      "Loss mean: -7.591452923949748\n",
      "Loss mean: -7.325835737788971\n",
      "Loss mean: -7.429176788630391\n",
      "Loss mean: -7.683986154576417\n",
      "Loss mean: -7.467437270001708\n",
      "Loss mean: -7.537634960605935\n",
      "Loss mean: -7.42313990387111\n",
      "Loss mean: -7.55515761388877\n",
      "Loss mean: -7.486195951266211\n",
      "Loss mean: -7.563674464264706\n",
      "Loss mean: -7.485024540557441\n",
      "Loss mean: -7.29158968140725\n",
      "Loss mean: -7.322524922104396\n",
      "Loss mean: -7.471282101907499\n",
      "Loss mean: -7.29158862952484\n",
      "Loss mean: -7.364750575542355\n",
      "Loss mean: -7.588426084047981\n",
      "Loss mean: -7.061528874663316\n",
      "Loss mean: -7.396232088658971\n",
      "Loss mean: -7.44162124298456\n",
      "Loss mean: -7.562215351321529\n",
      "Loss mean: -7.4843967310816675\n",
      "Loss mean: -7.432570938394422\n",
      "Loss mean: -7.366561182158485\n",
      "Loss mean: -7.3883559720737395\n",
      "Loss mean: -7.366552595922587\n",
      "Loss mean: -7.595032465585575\n",
      "Loss mean: -7.299021925866346\n",
      "Loss mean: -7.316999534360113\n",
      "Loss mean: -7.4873584379276235\n",
      "Loss mean: -7.111142741389113\n",
      "Loss mean: -7.402185031166691\n",
      "Loss mean: -7.574802152337737\n",
      "Loss mean: -7.466216212852473\n",
      "Loss mean: -7.41886677719322\n",
      "Loss mean: -7.590329358777797\n",
      "Loss mean: -7.511273258279953\n",
      "Loss mean: -7.4733461784961825\n",
      "Loss mean: -7.264179237875539\n",
      "Loss mean: -7.445629454159621\n",
      "Loss mean: -7.565576301945066\n",
      "Loss mean: -7.624540514846796\n",
      "Loss mean: -7.368961529666688\n",
      "Loss mean: -7.436616151138267\n",
      "Loss mean: -7.551139026158435\n",
      "Loss mean: -7.432286167221323\n",
      "Loss mean: -7.514943566060861\n",
      "Loss mean: -7.499633241245754\n",
      "Loss mean: -7.528554877697008\n",
      "Loss mean: -7.551594662938958\n",
      "Loss mean: -7.2774210715767715\n",
      "Loss mean: -7.649125651087572\n",
      "Loss mean: -7.632108717494375\n",
      "Loss mean: -7.364687341723364\n",
      "Loss mean: -7.196258680176723\n",
      "Loss mean: -7.474631997605685\n",
      "Loss mean: -7.366734790169432\n",
      "Loss mean: -7.587119467780563\n",
      "Loss mean: -7.406333256289463\n",
      "Loss mean: -7.66697175521357\n",
      "Loss mean: -7.157726403063157\n",
      "Loss mean: -7.280442709477443\n",
      "Loss mean: -7.410288191497197\n",
      "Loss mean: -7.500792314471346\n",
      "Loss mean: -7.604691213150783\n",
      "Loss mean: -7.595223627542378\n",
      "Loss mean: -7.423581293560486\n",
      "Loss mean: -7.399458859929639\n",
      "Loss mean: -7.554500248944019\n",
      "Loss mean: -7.451324550728597\n",
      "Loss mean: -7.762241859535998\n",
      "Loss mean: -7.40799097607973\n",
      "Loss mean: -7.554390564934275\n",
      "Loss mean: -7.438187391750322\n",
      "Loss mean: -7.528542452799805\n",
      "Loss mean: -7.480740615897762\n",
      "Loss mean: -7.361800145922781\n",
      "Loss mean: -7.504044604092432\n",
      "Loss mean: -7.482789470844286\n",
      "Loss mean: -7.329820106995354\n",
      "Loss mean: -7.322317189321427\n",
      "Loss mean: -7.382321700406533\n",
      "Loss mean: -7.504365191092469\n",
      "Loss mean: -7.420842471340329\n",
      "Loss mean: -7.405944179473581\n",
      "Loss mean: -7.484715920546873\n",
      "Loss mean: -7.225990116714391\n",
      "Loss mean: -7.52275032384061\n",
      "Loss mean: -7.2073439769089624\n",
      "Loss mean: -7.451895457121069\n",
      "Loss mean: -7.445578167940613\n",
      "Loss mean: -7.440070495469641\n",
      "Loss mean: -7.618830654560586\n",
      "Loss mean: -7.4765540855628565\n",
      "Loss mean: -7.335393698157728\n",
      "Loss mean: -7.403937436747424\n",
      "Loss mean: -7.181599229469558\n",
      "Loss mean: -7.536056516862617\n",
      "Loss mean: -7.261002462139755\n",
      "Loss mean: -7.393754559848653\n",
      "Loss mean: -7.172730820807902\n",
      "Loss mean: -7.490820012808498\n",
      "Loss mean: -7.4380482760540305\n",
      "Loss mean: -7.517125196014083\n",
      "Loss mean: -7.273645312493003\n",
      "Loss mean: -7.38337239510387\n",
      "Loss mean: -7.20783657172293\n",
      "Loss mean: -7.373170274309911\n",
      "Loss mean: -7.441735131209152\n",
      "Loss mean: -7.509894373385962\n",
      "Loss mean: -7.632247387085464\n",
      "Loss mean: -7.512304530962834\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m             mse \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreduce_mean(tf\u001b[39m.\u001b[39msquare(target_values \u001b[39m-\u001b[39m selected_q_values))\n\u001b[1;32m     45\u001b[0m             losses\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mmean(rewards))\n\u001b[0;32m---> 47\u001b[0m         gradients \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39;49mgradient(mse, Q_theta\u001b[39m.\u001b[39;49mtrainable_variables)\n\u001b[1;32m     48\u001b[0m         optimizer\u001b[39m.\u001b[39mapply_gradients(\u001b[39mzip\u001b[39m(gradients, Q_theta\u001b[39m.\u001b[39mtrainable_variables))\n\u001b[1;32m     50\u001b[0m \u001b[39m#print(MAX_STEPS)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/tensorflow/python/eager/backprop.py:1063\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1057\u001b[0m   output_gradients \u001b[39m=\u001b[39m (\n\u001b[1;32m   1058\u001b[0m       composite_tensor_gradient\u001b[39m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m   1059\u001b[0m           output_gradients))\n\u001b[1;32m   1060\u001b[0m   output_gradients \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m ops\u001b[39m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   1061\u001b[0m                       \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m output_gradients]\n\u001b[0;32m-> 1063\u001b[0m flat_grad \u001b[39m=\u001b[39m imperative_grad\u001b[39m.\u001b[39;49mimperative_grad(\n\u001b[1;32m   1064\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tape,\n\u001b[1;32m   1065\u001b[0m     flat_targets,\n\u001b[1;32m   1066\u001b[0m     flat_sources,\n\u001b[1;32m   1067\u001b[0m     output_gradients\u001b[39m=\u001b[39;49moutput_gradients,\n\u001b[1;32m   1068\u001b[0m     sources_raw\u001b[39m=\u001b[39;49mflat_sources_raw,\n\u001b[1;32m   1069\u001b[0m     unconnected_gradients\u001b[39m=\u001b[39;49munconnected_gradients)\n\u001b[1;32m   1071\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent:\n\u001b[1;32m   1072\u001b[0m   \u001b[39m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_watched_variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape\u001b[39m.\u001b[39mwatched_variables()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mUnknown value for unconnected_gradients: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_TapeGradient(\n\u001b[1;32m     68\u001b[0m     tape\u001b[39m.\u001b[39;49m_tape,  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m     target,\n\u001b[1;32m     70\u001b[0m     sources,\n\u001b[1;32m     71\u001b[0m     output_gradients,\n\u001b[1;32m     72\u001b[0m     sources_raw,\n\u001b[1;32m     73\u001b[0m     compat\u001b[39m.\u001b[39;49mas_str(unconnected_gradients\u001b[39m.\u001b[39;49mvalue))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/tensorflow/python/eager/backprop.py:146\u001b[0m, in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    144\u001b[0m     gradient_name_scope \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m forward_pass_name_scope \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[0;32m--> 146\u001b[0m     \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39;49mout_grads)\n\u001b[1;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m   \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39mout_grads)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/tensorflow/python/ops/nn_grad.py:348\u001b[0m, in \u001b[0;36m_BiasAddGrad\u001b[0;34m(op, received_grad)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m    346\u001b[0m   data_format \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \u001b[39mreturn\u001b[39;00m (received_grad,\n\u001b[0;32m--> 348\u001b[0m         gen_nn_ops\u001b[39m.\u001b[39;49mbias_add_grad(\n\u001b[1;32m    349\u001b[0m             out_backprop\u001b[39m=\u001b[39;49mreceived_grad, data_format\u001b[39m=\u001b[39;49mdata_format))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/tensorflow/python/ops/gen_nn_ops.py:924\u001b[0m, in \u001b[0;36mbias_add_grad\u001b[0;34m(out_backprop, data_format, name)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m    923\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 924\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[1;32m    925\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mBiasAddGrad\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, out_backprop, \u001b[39m\"\u001b[39;49m\u001b[39mdata_format\u001b[39;49m\u001b[39m\"\u001b[39;49m, data_format)\n\u001b[1;32m    926\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m    927\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_ENVS = 10\n",
    "envs = Environment(NUM_ENVS)\n",
    "\n",
    "BUFFER_SIZE = 100000\n",
    "buffer = Buffer(BUFFER_SIZE)\n",
    "\n",
    "MAX_STEPS = 1_000_000\n",
    "converged = False\n",
    "\n",
    "TAU = 0.5\n",
    "N = 5\n",
    "K = 3\n",
    "MINIBATCH_SIZE = 10\n",
    "GAMMA = 0.3\n",
    "mse = 0\n",
    "losses = []\n",
    "while not converged and MAX_STEPS > 0:\n",
    "    Q_target.set_weights((1 - TAU) * np.array(Q_target.get_weights()) + TAU * np.array(Q_theta.get_weights()))\n",
    "    if len(losses) > 1000:\n",
    "        print(\"Loss mean:\", np.mean(losses))\n",
    "        losses = []\n",
    "    for n in range(N):\n",
    "        \n",
    "        # sample\n",
    "        samples = envs.sample(Q_theta)\n",
    "        # add to buffer\n",
    "        buffer.add_to_buffer(samples)\n",
    "        for k in range(K):\n",
    "            # sample minibatch\n",
    "            minibatch = buffer.sample_minibatch(MINIBATCH_SIZE)\n",
    "            # unpack\n",
    "            old_states = np.array(list(minibatch[:, 0]))\n",
    "            actions = np.array(list(minibatch[:, 1]))\n",
    "            rewards = minibatch[:, 2]\n",
    "            new_states = minibatch[:, 3]\n",
    "            new_states = np.array(list(new_states))\n",
    "            Q_target_values = Q_target(new_states)\n",
    "            max_Q_target_values = np.array([max(action_values) for action_values in Q_target_values])\n",
    "            target_values = GAMMA * max_Q_target_values + rewards\n",
    "            #\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = Q_theta(old_states)\n",
    "                selected_q_values = tf.gather(predictions, actions, batch_dims=1)\n",
    "                mse = tf.reduce_mean(tf.square(target_values - selected_q_values))\n",
    "                losses.append(np.mean(rewards))\n",
    "            \n",
    "            gradients = tape.gradient(mse, Q_theta.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, Q_theta.trainable_variables))\n",
    "\n",
    "    #print(MAX_STEPS)\n",
    "    MAX_STEPS -= 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
