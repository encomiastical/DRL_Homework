{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "# Define the CNN model\n",
    "def create_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(8, activation='relu'),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(4, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "Q_theta = create_model()\n",
    "# Build the model to initialize the weights\n",
    "Q_theta.build(input_shape=(None, 8))\n",
    "\n",
    "# Copy the model\n",
    "Q_target = tf.keras.models.clone_model(Q_theta)\n",
    "Q_target.build(input_shape=(None, 8))\n",
    "Q_target.set_weights(Q_theta.get_weights())\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, number_environments, epsilon=0.2):\n",
    "        self.num_envs = number_environments\n",
    "        self.envs = gym.vector.make('LunarLander-v2',\n",
    "                                    num_envs=number_environments,\n",
    "                                    render_mode='human')\n",
    "        self.current_states, _ = self.envs.reset()\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "\n",
    "    def sample(self, model):\n",
    "        # gather q values for each action for each enironment\n",
    "        q_values = model(self.current_states)\n",
    "        # get actions with highest q values\n",
    "        actions = np.argmax(q_values, axis=1)\n",
    "        # replace chosen action with random action with probability of epsilon\n",
    "        actions = [np.random.choice(4) if np.random.rand() < self.epsilon else action for action in actions]\n",
    "        # take actions\n",
    "        new_states, rewards, _, _, _ = self.envs.step(actions)\n",
    "        old_states = self.current_states\n",
    "        self.current_states = new_states\n",
    "        return old_states, actions, rewards, new_states\n",
    "    \n",
    "\n",
    "\n",
    "class Buffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = []\n",
    "    \n",
    "\n",
    "    def add_to_buffer(self, samples):\n",
    "        for i in range(len(samples[0])):\n",
    "            state = samples[0][i]\n",
    "            action = samples[1][i]\n",
    "            reward = samples[2][i]\n",
    "            new_state = samples[3][i]\n",
    "            sample = np.array([state, action, reward, new_state])\n",
    "            self.buffer.append(sample)\n",
    "        if len(self.buffer) > self.buffer_size:\n",
    "            self.buffer = self.buffer[-self.buffer_size:]\n",
    "    \n",
    "\n",
    "    def sample_minibatch(self, batch_size):\n",
    "        batch_size = min(batch_size, len(self.buffer))\n",
    "        minibatch = random.sample(self.buffer, batch_size)\n",
    "        return np.array(minibatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m NUM_ENVS \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[0;32m----> 2\u001b[0m envs \u001b[39m=\u001b[39m Environment(NUM_ENVS)\n\u001b[1;32m      4\u001b[0m BUFFER_SIZE \u001b[39m=\u001b[39m \u001b[39m100000\u001b[39m\n\u001b[1;32m      5\u001b[0m buffer \u001b[39m=\u001b[39m Buffer(BUFFER_SIZE)\n",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m, in \u001b[0;36mEnvironment.__init__\u001b[0;34m(self, number_environments, epsilon)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, number_environments, epsilon\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs \u001b[39m=\u001b[39m number_environments\n\u001b[0;32m----> 4\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvs \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39;49mvector\u001b[39m.\u001b[39;49mmake(\u001b[39m'\u001b[39;49m\u001b[39mLunarLander-v2\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m                                 num_envs\u001b[39m=\u001b[39;49mnumber_environments,\n\u001b[1;32m      6\u001b[0m                                 render_mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mhuman\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_states, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvs\u001b[39m.\u001b[39mreset()\n\u001b[1;32m      8\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon \u001b[39m=\u001b[39m epsilon\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/gymnasium/vector/__init__.py:82\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, num_envs, asynchronous, wrappers, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[39mreturn\u001b[39;00m _make_env\n\u001b[1;32m     79\u001b[0m env_fns \u001b[39m=\u001b[39m [\n\u001b[1;32m     80\u001b[0m     create_env(disable_env_checker \u001b[39mor\u001b[39;00m env_num \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m env_num \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_envs)\n\u001b[1;32m     81\u001b[0m ]\n\u001b[0;32m---> 82\u001b[0m \u001b[39mreturn\u001b[39;00m AsyncVectorEnv(env_fns) \u001b[39mif\u001b[39;00m asynchronous \u001b[39melse\u001b[39;00m SyncVectorEnv(env_fns)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:169\u001b[0m, in \u001b[0;36mAsyncVectorEnv.__init__\u001b[0;34m(self, env_fns, observation_space, action_space, shared_memory, copy, context, daemon, worker)\u001b[0m\n\u001b[1;32m    166\u001b[0m         child_pipe\u001b[39m.\u001b[39mclose()\n\u001b[1;32m    168\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m=\u001b[39m AsyncState\u001b[39m.\u001b[39mDEFAULT\n\u001b[0;32m--> 169\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_spaces()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:504\u001b[0m, in \u001b[0;36mAsyncVectorEnv._check_spaces\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[39mfor\u001b[39;00m pipe \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparent_pipes:\n\u001b[1;32m    503\u001b[0m     pipe\u001b[39m.\u001b[39msend((\u001b[39m\"\u001b[39m\u001b[39m_check_spaces\u001b[39m\u001b[39m\"\u001b[39m, spaces))\n\u001b[0;32m--> 504\u001b[0m results, successes \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[pipe\u001b[39m.\u001b[39;49mrecv() \u001b[39mfor\u001b[39;49;00m pipe \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparent_pipes])\n\u001b[1;32m    505\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_if_errors(successes)\n\u001b[1;32m    506\u001b[0m same_observation_spaces, same_action_spaces \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mresults)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:504\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[39mfor\u001b[39;00m pipe \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparent_pipes:\n\u001b[1;32m    503\u001b[0m     pipe\u001b[39m.\u001b[39msend((\u001b[39m\"\u001b[39m\u001b[39m_check_spaces\u001b[39m\u001b[39m\"\u001b[39m, spaces))\n\u001b[0;32m--> 504\u001b[0m results, successes \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[pipe\u001b[39m.\u001b[39;49mrecv() \u001b[39mfor\u001b[39;00m pipe \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparent_pipes])\n\u001b[1;32m    505\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_if_errors(successes)\n\u001b[1;32m    506\u001b[0m same_observation_spaces, same_action_spaces \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mresults)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.2_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py:249\u001b[0m, in \u001b[0;36m_ConnectionBase.recv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[1;32m    248\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n\u001b[0;32m--> 249\u001b[0m buf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_recv_bytes()\n\u001b[1;32m    250\u001b[0m \u001b[39mreturn\u001b[39;00m _ForkingPickler\u001b[39m.\u001b[39mloads(buf\u001b[39m.\u001b[39mgetbuffer())\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.2_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py:413\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_recv_bytes\u001b[39m(\u001b[39mself\u001b[39m, maxsize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 413\u001b[0m     buf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_recv(\u001b[39m4\u001b[39;49m)\n\u001b[1;32m    414\u001b[0m     size, \u001b[39m=\u001b[39m struct\u001b[39m.\u001b[39munpack(\u001b[39m\"\u001b[39m\u001b[39m!i\u001b[39m\u001b[39m\"\u001b[39m, buf\u001b[39m.\u001b[39mgetvalue())\n\u001b[1;32m    415\u001b[0m     \u001b[39mif\u001b[39;00m size \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.2_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py:378\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    376\u001b[0m remaining \u001b[39m=\u001b[39m size\n\u001b[1;32m    377\u001b[0m \u001b[39mwhile\u001b[39;00m remaining \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 378\u001b[0m     chunk \u001b[39m=\u001b[39m read(handle, remaining)\n\u001b[1;32m    379\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(chunk)\n\u001b[1;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m n \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_ENVS = 5\n",
    "envs = Environment(NUM_ENVS)\n",
    "\n",
    "BUFFER_SIZE = 100000\n",
    "buffer = Buffer(BUFFER_SIZE)\n",
    "\n",
    "MAX_STEPS = 1_000_000\n",
    "converged = False\n",
    "\n",
    "TAU = 0.5\n",
    "N = 5\n",
    "K = 3\n",
    "MINIBATCH_SIZE = 10\n",
    "GAMMA = 0.3\n",
    "mse = 0\n",
    "losses = []\n",
    "while not converged and MAX_STEPS > 0:\n",
    "    Q_target.set_weights((1 - TAU) * np.array(Q_target.get_weights()) + TAU * np.array(Q_theta.get_weights()))\n",
    "    if len(losses) > 1000:\n",
    "        print(\"Loss mean:\", np.mean(losses))\n",
    "        losses = []\n",
    "    for n in range(N):\n",
    "        \n",
    "        # sample\n",
    "        samples = envs.sample(Q_theta)\n",
    "        # add to buffer\n",
    "        buffer.add_to_buffer(samples)\n",
    "        for k in range(K):\n",
    "            # sample minibatch\n",
    "            minibatch = buffer.sample_minibatch(MINIBATCH_SIZE)\n",
    "            # unpack\n",
    "            old_states = np.array(list(minibatch[:, 0]))\n",
    "            actions = np.array(list(minibatch[:, 1]))\n",
    "            rewards = minibatch[:, 2]\n",
    "            new_states = minibatch[:, 3]\n",
    "            new_states = np.array(list(new_states))\n",
    "            Q_target_values = Q_target(new_states)\n",
    "            max_Q_target_values = np.array([max(action_values) for action_values in Q_target_values])\n",
    "            target_values = GAMMA * max_Q_target_values + rewards\n",
    "            #\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = Q_theta(old_states)\n",
    "                selected_q_values = tf.gather(predictions, actions, batch_dims=1)\n",
    "                mse = tf.reduce_mean(tf.square(target_values - selected_q_values))\n",
    "                losses.append(np.mean(rewards))\n",
    "            \n",
    "            gradients = tape.gradient(mse, Q_theta.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, Q_theta.trainable_variables))\n",
    "\n",
    "    print(MAX_STEPS)\n",
    "    MAX_STEPS -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 16:57:47.533 Python[64693:4478232] ApplePersistenceIgnoreState: Existing state will not be touched. New state will be written to /var/folders/mt/0n8phlt50rn7h35xhgh97mw80000gn/T/org.python.python.savedState\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "max_steps = 1_000_000\n",
    "N = 10\n",
    "K = 2\n",
    "EPSILON = 0.2\n",
    "MINI_BATCH_SIZE = 10\n",
    "GAMMA = 0.5\n",
    "TAU = 0.9\n",
    "converged = False\n",
    "\n",
    "# Create the replay buffer\n",
    "buffer_size = 10000  # Define the size of the replay buffer\n",
    "buffer = []#deque(maxlen=buffer_size)\n",
    "\n",
    "observation, info = envs.reset()\n",
    "print(observation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
