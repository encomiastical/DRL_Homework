{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 13:57:57.678752: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-14 13:57:58.775569: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the CNN model\n",
    "def create_model(regularization_factor):\n",
    "    #inputs = tf.keras.Input(shape=(4,))\n",
    "    inputs = tf.keras.Input(shape=(8,)) #changed\n",
    "    x = tf.keras.layers.Dense(64, activation=\"relu\", kernel_regularizer=l2(regularization_factor))(inputs)\n",
    "    x = tf.keras.layers.Dense(64, activation=\"relu\", kernel_regularizer=l2(regularization_factor))(x)\n",
    "    x = tf.keras.layers.Dense(64, activation=\"relu\", kernel_regularizer=l2(regularization_factor))(x)\n",
    "    #outputs = tf.keras.layers.Dense(2, activation=\"relu\")(x)\n",
    "    outputs = tf.keras.layers.Dense(4, activation=\"relu\")(x) #changed\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"LunarLander\")\n",
    "    return model\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, NUM_ENVS):\n",
    "        self.num_envs = NUM_ENVS\n",
    "        #self.envs = envs = gym.vector.make('CartPole-v1', num_envs=NUM_ENVS) #changed\n",
    "        self.envs = envs = gym.vector.make('LunarLander-v2', num_envs=NUM_ENVS)\n",
    "        self.current_state, _ = self.envs.reset()\n",
    "\n",
    "    def sample(self, model, epsilon):\n",
    "        q_values = model(self.current_state) #get q values for current state\n",
    "        action = np.argmax(q_values, axis=1) #get action that maximizes q-value\n",
    "        action = [self.envs.single_action_space.sample() if random.random() < epsilon else a for a in action] #choose epsilon greedy\n",
    "        new_observation, reward, terminated, _, _ = self.envs.step(action)\n",
    "        old_observation = self.current_state\n",
    "\n",
    "        self.current_state = new_observation #update current state after environment did step\n",
    "        return (old_observation, action, reward, new_observation, terminated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = []\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def add_to_buffer(self, samples):\n",
    "        #unpack the different environments\n",
    "        old_obs_ = samples[0]\n",
    "        actions_ = samples[1]\n",
    "        rewards_ = samples[2]\n",
    "        new_obs_ = samples[3]\n",
    "        terminateds_ = samples[4]\n",
    "        for o, a, r, no, t in zip(old_obs_, actions_, rewards_, new_obs_, terminateds_):\n",
    "            self.buffer.append((o, a, r, no, t))\n",
    "        if len(self.buffer) > self.max_size:\n",
    "            self.buffer = self.buffer[-self.max_size:]\n",
    "\n",
    "    def sample_minibatch(self, batch_size):\n",
    "        r = random.sample(self.buffer, min(len(self.buffer), batch_size))\n",
    "        return r\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50 #outer loop\n",
    "K = 3 #inner loop\n",
    "MINI_BATCH_SIZE = 64\n",
    "CONVERGED = False\n",
    "MAX_STEPS = 300\n",
    "GAMMA = 0.99\n",
    "regularization_factor = 0.001\n",
    "epsilon = 1.0\n",
    "minimum_epsilon = 0.01\n",
    "epsilon_decay = 0.99\n",
    "NUM_ENVS = 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average return since last print: -143.0058446458182 in step 10\n",
      "average return since last print: -115.80734770275289 in step 20\n",
      "average return since last print: -85.7338471485924 in step 30\n",
      "average return since last print: -68.57302522665206 in step 40\n",
      "average return since last print: -71.17965163612428 in step 50\n",
      "average return since last print: -63.02945402073765 in step 60\n",
      "average return since last print: -57.46632731007702 in step 70\n",
      "average return since last print: -62.07379209686795 in step 80\n",
      "average return since last print: -34.627443484797304 in step 90\n",
      "average return since last print: -7.07704475558142 in step 100\n",
      "average return since last print: -6.9286183196996625 in step 110\n",
      "average return since last print: -20.266930050469448 in step 120\n",
      "average return since last print: -14.365178384422734 in step 130\n",
      "average return since last print: -13.430228135045498 in step 140\n",
      "average return since last print: 20.600129617568747 in step 150\n",
      "average return since last print: 75.25735588537918 in step 160\n",
      "average return since last print: 29.920183369945164 in step 170\n",
      "average return since last print: 217.29143986299218 in step 180\n",
      "average return since last print: 239.12391571037546 in step 190\n",
      "average return since last print: 133.12933422915535 in step 200\n",
      "average return since last print: 54.5033921047083 in step 210\n",
      "average return since last print: 128.61128110176497 in step 220\n",
      "no new returns in step 230\n",
      "no new returns in step 240\n",
      "no new returns in step 250\n",
      "no new returns in step 260\n",
      "average return since last print: -38.129180545222525 in step 270\n",
      "no new returns in step 280\n",
      "no new returns in step 290\n",
      "no new returns in step 300\n"
     ]
    }
   ],
   "source": [
    "#init environments\n",
    "envs = Environment(NUM_ENVS)\n",
    "\n",
    "#just some approximatory metrics\n",
    "returns = np.zeros(NUM_ENVS)\n",
    "return_tracker = []\n",
    "\n",
    "# Create the model\n",
    "Q_theta = create_model(regularization_factor)\n",
    "Q_target = tf.keras.models.clone_model(Q_theta)\n",
    "\n",
    "# init buffer\n",
    "kabuff = Buffer(100000)\n",
    "\n",
    "STEPS = 0\n",
    "\n",
    "while not CONVERGED and STEPS < MAX_STEPS:\n",
    "    STEPS += 1\n",
    "    \n",
    "    Q_target.set_weights(Q_theta.get_weights()) #update target network to current training network interation\n",
    "    epsilon = max(minimum_epsilon, epsilon*epsilon_decay) #anneal the epsilon used for sampling\n",
    "    \n",
    "    for n in range(N):\n",
    "        \n",
    "        new_samples = envs.sample(Q_theta, epsilon) #sample from the environment\n",
    "        \n",
    "        #update return-metrics, reset returns if terminated\n",
    "        #envs.sample returns a tuple of NUM_ENVS long np.arrays corresponding to old_obs, actions, rewards, new_obs, terminated\n",
    "        #we add the rewards to our return counters\n",
    "        returns += new_samples[2] #new_samples[2] = rewards\n",
    "        for i, t in enumerate(new_samples[4]): #new_samples[4] = terminateds\n",
    "            if t:\n",
    "                #environment terminated, add to tracker and reset\n",
    "                return_tracker.append(returns[i])\n",
    "                returns[i] = 0\n",
    "\n",
    "        kabuff.add_to_buffer(new_samples) #add environment samples to the buffer\n",
    "\n",
    "        for k in range(K):\n",
    "            #sample s,a,r,s' minibatch from buffer\n",
    "            minibatch = kabuff.sample_minibatch(MINI_BATCH_SIZE)\n",
    "\n",
    "            #unpack the minibatch\n",
    "            new_states = np.array([sample[3] for sample in minibatch])\n",
    "            rewards = np.array([sample[2] for sample in minibatch])\n",
    "            actions = np.array([sample[1] for sample in minibatch])\n",
    "            old_states = np.array([sample[0] for sample in minibatch])\n",
    "            terminateds = np.array([sample[4] for sample in minibatch])\n",
    "\n",
    "            #calculate the targets, don't add new Q_target_values if an environment just terminated\n",
    "            Q_target_values = Q_target(new_states)\n",
    "            max_Q_target_values = np.array([max(action_values) for action_values in Q_target_values])\n",
    "            target_q_values = rewards + GAMMA * max_Q_target_values * (1-terminateds)\n",
    "\n",
    "            #training: do gradient descent of Q_theta in direction of target\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = Q_theta(old_states)\n",
    "                selected_q_values = tf.gather(predictions, actions, batch_dims=1) #predictions of Q(old_states, actions)\n",
    "                loss_value = tf.reduce_mean(tf.square(selected_q_values-target_q_values)) #compute simple MSE loss\n",
    "            gradients = tape.gradient(loss_value, Q_theta.trainable_variables) #get\n",
    "            optimizer.apply_gradients(zip(gradients, Q_theta.trainable_variables)) #and apply gradients\n",
    "\n",
    "    #every 10 steps, print log message and reset returns\n",
    "    if STEPS % 10 == 0:\n",
    "        if(return_tracker):\n",
    "            print(\"average return since last print: \" + str(np.mean(return_tracker)) + \" in step \" + str(STEPS))\n",
    "            return_tracker = []\n",
    "        else:\n",
    "            print(\"no new returns in step \" + str(STEPS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: fourth_try/assets\n"
     ]
    }
   ],
   "source": [
    "Q_theta.save('fourth_try')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "Q_theta = tf.keras.models.load_model(\"third_try\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = gym.make('LunarLander-v2', render_mode='human')\n",
    "#test_env = gym.make('CartPole-v1', render_mode='human')\n",
    "obs, inf = test_env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    qs = Q_theta(tf.expand_dims(obs, 0))\n",
    "    act = np.argmax(qs)\n",
    "    obs, _, terminated, _, _ = test_env.step(act)\n",
    "    if(terminated):\n",
    "        obs, _ = test_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
