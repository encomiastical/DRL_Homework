{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-16 09:36:59.579834: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "\n",
    "N = 50 #outer loop\n",
    "K = 2 #inner loop\n",
    "MINI_BATCH_SIZE = 64\n",
    "CONVERGED = False\n",
    "MAX_STEPS = 1000\n",
    "GAMMA = 0.99\n",
    "regularization_factor = 0.001\n",
    "epsilon = 1.0\n",
    "minimum_epsilon = 0.02\n",
    "epsilon_decay = 0.992\n",
    "NUM_ENVS = 5\n",
    "BUFFER_SIZE = 100000\n",
    "\n",
    "\n",
    "use_prefill = False\n",
    "use_DDQN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-16 09:37:00.702519: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "def create_model(regularization_factor):\n",
    "    #inputs = tf.keras.Input(shape=(4,))\n",
    "    inputs = tf.keras.Input(shape=(8,)) #changed\n",
    "    x = tf.keras.layers.Dense(64, activation=\"relu\", kernel_regularizer=l2(regularization_factor))(inputs)\n",
    "    x = tf.keras.layers.Dense(64, activation=\"relu\", kernel_regularizer=l2(regularization_factor))(x)\n",
    "    x = tf.keras.layers.Dense(64, activation=\"relu\", kernel_regularizer=l2(regularization_factor))(x)\n",
    "    #outputs = tf.keras.layers.Dense(2, activation=\"relu\")(x)\n",
    "    outputs = tf.keras.layers.Dense(4, activation=\"relu\")(x) #changed\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"LunarLander\")\n",
    "    return model\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, NUM_ENVS):\n",
    "        self.num_envs = NUM_ENVS\n",
    "        #self.envs = envs = gym.vector.make('CartPole-v1', num_envs=NUM_ENVS) #changed\n",
    "        self.envs = envs = gym.vector.make('LunarLander-v2', num_envs=NUM_ENVS)\n",
    "        self.envs.single_action_space.seed(11)\n",
    "        self.current_state, _ = self.envs.reset(seed=11)\n",
    "\n",
    "    def sample(self, model, epsilon):\n",
    "        q_values = model(self.current_state) #get q values for current state\n",
    "        action = np.argmax(q_values, axis=1) #get action that maximizes q-value\n",
    "        action = [self.envs.single_action_space.sample() if random.random() < epsilon else a for a in action] #choose epsilon greedy\n",
    "        new_observation, reward, terminated, _, _ = self.envs.step(action)\n",
    "        old_observation = self.current_state\n",
    "\n",
    "        self.current_state = new_observation #update current state after environment did step\n",
    "        return (old_observation, action, reward, new_observation, terminated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, max_size, path=None):\n",
    "        self.buffer = []\n",
    "        self.max_size = max_size\n",
    "        if path:\n",
    "            self.load_from_file(path)\n",
    "\n",
    "    def add_to_buffer(self, samples):\n",
    "        #unpack the different environments\n",
    "        old_obs_ = samples[0]\n",
    "        actions_ = samples[1]\n",
    "        rewards_ = samples[2]\n",
    "        new_obs_ = samples[3]\n",
    "        terminateds_ = samples[4]\n",
    "        for o, a, r, no, t in zip(old_obs_, actions_, rewards_, new_obs_, terminateds_):\n",
    "            self.buffer.append((o, a, r, no, t))\n",
    "        if len(self.buffer) > self.max_size:\n",
    "            self.buffer = self.buffer[-self.max_size:]\n",
    "\n",
    "    def sample_minibatch(self, batch_size):\n",
    "        r = random.sample(self.buffer, min(len(self.buffer), batch_size))\n",
    "        return r\n",
    "    \n",
    "    def load_from_file(self, path):\n",
    "        with open(path, 'rb') as f:\n",
    "            lst = pickle.load(f)\n",
    "        self.buffer += lst\n",
    "        if len(self.buffer) > self.max_size:\n",
    "            self.buffer = self.buffer[-self.max_size:]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "average return since last print: -164.94992224754242 in step 10\n",
      "average return since last print: -142.78575830509104 in step 20\n",
      "average return since last print: -115.13286477942938 in step 30\n",
      "average return since last print: -105.14959073397694 in step 40\n",
      "average return since last print: -75.29200772365354 in step 50\n",
      "average return since last print: -68.03947969151122 in step 60\n",
      "average return since last print: -80.53609298008551 in step 70\n",
      "average return since last print: -71.14641169652228 in step 80\n",
      "average return since last print: -78.86853927395379 in step 90\n",
      "average return since last print: -35.719391516568194 in step 100\n",
      "average return since last print: -32.491825232217785 in step 110\n",
      "average return since last print: -32.73421892419713 in step 120\n",
      "average return since last print: -18.15322176594415 in step 130\n",
      "average return since last print: -38.8963417287224 in step 140\n",
      "average return since last print: -80.4462782196038 in step 150\n",
      "average return since last print: -58.38825297581343 in step 160\n",
      "average return since last print: -10.134210666157353 in step 170\n",
      "average return since last print: -74.100372341013 in step 180\n",
      "average return since last print: -34.440196593843496 in step 190\n",
      "no new returns in step 200\n",
      "no new returns in step 210\n",
      "average return since last print: -441.02568689482564 in step 220\n",
      "no new returns in step 230\n",
      "no new returns in step 240\n",
      "no new returns in step 250\n",
      "no new returns in step 260\n",
      "average return since last print: -56.88622663251067 in step 270\n",
      "no new returns in step 280\n",
      "average return since last print: 189.44792946093423 in step 290\n",
      "average return since last print: 139.5761079510496 in step 300\n",
      "average return since last print: 243.39878030608793 in step 310\n",
      "average return since last print: 167.9645625905781 in step 320\n",
      "average return since last print: 93.03018897764099 in step 330\n",
      "average return since last print: 215.44572603336266 in step 340\n",
      "average return since last print: 82.32222866821833 in step 350\n",
      "average return since last print: 79.11473103265656 in step 360\n",
      "average return since last print: 164.36247721937303 in step 370\n",
      "average return since last print: -8.093274886180586 in step 380\n",
      "average return since last print: 103.21107528311846 in step 390\n",
      "average return since last print: 165.03257108089372 in step 400\n",
      "average return since last print: 165.4954401175479 in step 410\n",
      "average return since last print: 179.76592896490558 in step 420\n",
      "average return since last print: 131.78513077441093 in step 430\n",
      "average return since last print: 219.70125103152222 in step 440\n",
      "average return since last print: 166.94484037404203 in step 450\n",
      "average return since last print: 178.02548670270048 in step 460\n",
      "average return since last print: 227.166303776137 in step 470\n",
      "average return since last print: 205.83980551306792 in step 480\n",
      "average return since last print: 200.97580790672274 in step 490\n",
      "average return since last print: 222.6152752910629 in step 500\n",
      "average return since last print: 233.3761717728276 in step 510\n",
      "average return since last print: 82.97764020158714 in step 520\n",
      "average return since last print: 178.9212871247494 in step 530\n",
      "average return since last print: 104.51360123090583 in step 540\n",
      "average return since last print: 189.4161753452362 in step 550\n",
      "average return since last print: 253.24480138417576 in step 560\n",
      "average return since last print: 260.45756358084344 in step 570\n",
      "average return since last print: 254.98308224558576 in step 580\n",
      "average return since last print: 267.0325331013496 in step 590\n",
      "average return since last print: 261.1035276586612 in step 600\n",
      "average return since last print: 256.5834082617515 in step 610\n",
      "average return since last print: 232.7620296435463 in step 620\n",
      "average return since last print: 254.5833557759529 in step 630\n",
      "average return since last print: 244.34413148694242 in step 640\n",
      "average return since last print: 232.4848248916993 in step 650\n",
      "average return since last print: 267.21230084382984 in step 660\n",
      "average return since last print: 285.97465359446034 in step 670\n",
      "average return since last print: 220.0656715201142 in step 680\n",
      "average return since last print: 229.47605445884182 in step 690\n",
      "average return since last print: 271.0361408784296 in step 700\n",
      "average return since last print: 253.98649685851063 in step 710\n",
      "average return since last print: 248.2513615192642 in step 720\n",
      "average return since last print: 273.7162191188208 in step 730\n",
      "average return since last print: 266.0229558060895 in step 740\n",
      "average return since last print: 293.4779878681527 in step 750\n",
      "average return since last print: 242.21688650877573 in step 760\n",
      "average return since last print: 249.61855414091644 in step 770\n",
      "average return since last print: 198.48143099676665 in step 780\n",
      "average return since last print: 229.22587270461017 in step 790\n",
      "average return since last print: 238.67111691455406 in step 800\n",
      "average return since last print: 212.05313136918664 in step 810\n",
      "average return since last print: 248.9126787712045 in step 820\n",
      "average return since last print: 210.2782884344518 in step 830\n",
      "average return since last print: 239.531519434114 in step 840\n",
      "average return since last print: 128.65371981983645 in step 850\n",
      "average return since last print: 266.4401562281582 in step 860\n",
      "average return since last print: 271.6903674925526 in step 870\n",
      "average return since last print: 254.57457628959256 in step 880\n",
      "average return since last print: 214.6104915782806 in step 890\n",
      "average return since last print: 245.64743167457476 in step 900\n",
      "average return since last print: 235.82319761600195 in step 910\n",
      "average return since last print: 194.81578646179423 in step 920\n",
      "average return since last print: 138.50027114723565 in step 930\n",
      "average return since last print: 139.21884430497434 in step 940\n",
      "average return since last print: 115.6799275068771 in step 950\n",
      "average return since last print: 156.4092352216252 in step 960\n",
      "average return since last print: 173.16439680995796 in step 970\n",
      "average return since last print: 193.8228549117912 in step 980\n",
      "average return since last print: 191.75915720964105 in step 990\n",
      "average return since last print: 264.3427364863826 in step 1000\n"
     ]
    }
   ],
   "source": [
    "#init environments\n",
    "envs = Environment(NUM_ENVS)\n",
    "\n",
    "#just some approximatory metrics\n",
    "returns = np.zeros(NUM_ENVS)\n",
    "return_tracker = []\n",
    "\n",
    "# Create the model\n",
    "#Q_theta = create_model(regularization_factor)\n",
    "Q_theta = tf.keras.models.load_model(\"final_model\")\n",
    "Q_target = tf.keras.models.clone_model(Q_theta)\n",
    "\n",
    "# init buffer\n",
    "kabuff = Buffer(BUFFER_SIZE)\n",
    "# prefill buffer\n",
    "if use_prefill:\n",
    "    kabuff.load_from_file(\"kabuff.pkl\") #kabuff.pkl contains samples derived from a previous working policy\n",
    "\n",
    "STEPS = 0\n",
    "\n",
    "while not CONVERGED and STEPS < MAX_STEPS:\n",
    "    STEPS += 1\n",
    "    \n",
    "    Q_target.set_weights(Q_theta.get_weights()) #update target network to current training network interation\n",
    "    epsilon = max(minimum_epsilon, epsilon*epsilon_decay) #anneal the epsilon used for sampling\n",
    "    \n",
    "    for n in range(N):\n",
    "        \n",
    "        new_samples = envs.sample(Q_theta, epsilon) #sample from the environment\n",
    "        \n",
    "        #update return-metrics, reset returns if terminated\n",
    "        #envs.sample returns a tuple of NUM_ENVS long np.arrays corresponding to old_obs, actions, rewards, new_obs, terminated\n",
    "        #we add the rewards to our return counters\n",
    "        returns += new_samples[2] #new_samples[2] = rewards\n",
    "        for i, t in enumerate(new_samples[4]): #new_samples[4] = terminateds\n",
    "            if t:\n",
    "                #environment terminated, add to tracker and reset\n",
    "                return_tracker.append(returns[i])\n",
    "                returns[i] = 0\n",
    "\n",
    "        kabuff.add_to_buffer(new_samples) #add environment samples to the buffer\n",
    "\n",
    "        for k in range(K):\n",
    "            #sample s,a,r,s' minibatch from buffer\n",
    "            minibatch = kabuff.sample_minibatch(MINI_BATCH_SIZE)\n",
    "\n",
    "            #unpack the minibatch\n",
    "            new_states = np.array([sample[3] for sample in minibatch])\n",
    "            rewards = np.array([sample[2] for sample in minibatch])\n",
    "            actions = np.array([sample[1] for sample in minibatch])\n",
    "            old_states = np.array([sample[0] for sample in minibatch])\n",
    "            terminateds = np.array([sample[4] for sample in minibatch])\n",
    "\n",
    "\n",
    "            #DDQN\n",
    "            if use_DDQN:\n",
    "                Q_target_values = Q_target(new_states)\n",
    "                Q_theta_values = Q_theta(new_states)\n",
    "                max_Q_target_values = []\n",
    "                for target_action_vals, theta_action_vals in zip(Q_target_values, Q_theta_values):\n",
    "                    Q_theta_max_index = np.argmax(theta_action_vals)\n",
    "                    max_Q_target_value = target_action_vals[Q_theta_max_index]\n",
    "                    max_Q_target_values.append(max_Q_target_value)\n",
    "                max_Q_target_values = np.array(max_Q_target_values)\n",
    "            else:\n",
    "                Q_target_values = Q_target(new_states)\n",
    "                max_Q_target_values = np.array([max(action_values) for action_values in Q_target_values])\n",
    "\n",
    "\n",
    "            #calculate the targets, don't add new Q_target_values if an environment just terminated\n",
    "            target_q_values = rewards + GAMMA * max_Q_target_values * (1-terminateds)\n",
    "\n",
    "            #training: do gradient descent of Q_theta in direction of target\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = Q_theta(old_states)\n",
    "                selected_q_values = tf.gather(predictions, actions, batch_dims=1) #predictions of Q(old_states, actions)\n",
    "                loss_value = tf.reduce_mean(tf.square(selected_q_values-target_q_values)) #compute simple MSE loss\n",
    "            gradients = tape.gradient(loss_value, Q_theta.trainable_variables) #get\n",
    "            optimizer.apply_gradients(zip(gradients, Q_theta.trainable_variables)) #and apply gradients\n",
    "\n",
    "    #every 10 steps, print log message and reset returns\n",
    "    if STEPS % 10 == 0:\n",
    "        if(return_tracker):\n",
    "            print(\"average return since last print: \" + str(np.mean(return_tracker)) + \" in step \" + str(STEPS))\n",
    "            return_tracker = []\n",
    "        else:\n",
    "            print(\"no new returns in step \" + str(STEPS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: final_model_/assets\n"
     ]
    }
   ],
   "source": [
    "Q_theta.save('final_model_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "Q_theta = tf.keras.models.load_model(\"final_model_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = gym.make('LunarLander-v2', render_mode='human')\n",
    "#test_env = gym.make('CartPole-v1', render_mode='human')\n",
    "obs, inf = test_env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_env.reset()\n",
    "for i in range(1000):\n",
    "    qs = Q_theta(tf.expand_dims(obs, 0))\n",
    "    act = np.argmax(qs)\n",
    "    obs, _, terminated, _, _ = test_env.step(act)\n",
    "    if(terminated):\n",
    "        obs, _ = test_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
