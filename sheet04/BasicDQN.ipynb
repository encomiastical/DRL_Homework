{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "#tf.random.set_seed(7)\n",
    "#np.random.seed(42)\n",
    "#random.seed(1337)\n",
    "#setting seeds doesn't really work, maybe gym uses some hidden seeds?\n",
    "#add env.action_space.seed(RANDOM_SEED) ??\n",
    "\n",
    "\n",
    "N = 10 #outer loop\n",
    "K = 3 #inner loop\n",
    "MINI_BATCH_SIZE = 20\n",
    "CONVERGED = False\n",
    "MAX_STEPS = 4000\n",
    "GAMMA = 0.99\n",
    "regularization_factor = 0.001\n",
    "epsilon = 1.0\n",
    "minimum_epsilon = 0.01\n",
    "epsilon_decay = 0.9994\n",
    "NUM_ENVS = 3\n",
    "BUFFER_SIZE = 100000\n",
    "\n",
    "\n",
    "use_prefill = True\n",
    "use_DDQN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "def create_model(regularization_factor):\n",
    "    #inputs = tf.keras.Input(shape=(4,))\n",
    "    inputs = tf.keras.Input(shape=(8,)) #changed\n",
    "    x = tf.keras.layers.Dense(64, activation=\"relu\", kernel_regularizer=l2(regularization_factor))(inputs)\n",
    "    x = tf.keras.layers.Dense(64, activation=\"relu\", kernel_regularizer=l2(regularization_factor))(x)\n",
    "    x = tf.keras.layers.Dense(64, activation=\"relu\", kernel_regularizer=l2(regularization_factor))(x)\n",
    "    #outputs = tf.keras.layers.Dense(2, activation=\"relu\")(x)\n",
    "    outputs = tf.keras.layers.Dense(4, activation=\"relu\")(x) #changed\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"LunarLander\")\n",
    "    return model\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, NUM_ENVS):\n",
    "        self.num_envs = NUM_ENVS\n",
    "        #self.envs = envs = gym.vector.make('CartPole-v1', num_envs=NUM_ENVS) #changed\n",
    "        self.envs = envs = gym.vector.make('LunarLander-v2', num_envs=NUM_ENVS)\n",
    "        self.current_state, _ = self.envs.reset(seed=11)\n",
    "\n",
    "    def sample(self, model, epsilon):\n",
    "        q_values = model(self.current_state) #get q values for current state\n",
    "        action = np.argmax(q_values, axis=1) #get action that maximizes q-value\n",
    "        action = [self.envs.single_action_space.sample() if random.random() < epsilon else a for a in action] #choose epsilon greedy\n",
    "        new_observation, reward, terminated, _, _ = self.envs.step(action)\n",
    "        old_observation = self.current_state\n",
    "\n",
    "        self.current_state = new_observation #update current state after environment did step\n",
    "        return (old_observation, action, reward, new_observation, terminated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, max_size, path=None):\n",
    "        self.buffer = []\n",
    "        self.max_size = max_size\n",
    "        if path:\n",
    "            self.load_from_file(path)\n",
    "\n",
    "    def add_to_buffer(self, samples):\n",
    "        #unpack the different environments\n",
    "        old_obs_ = samples[0]\n",
    "        actions_ = samples[1]\n",
    "        rewards_ = samples[2]\n",
    "        new_obs_ = samples[3]\n",
    "        terminateds_ = samples[4]\n",
    "        for o, a, r, no, t in zip(old_obs_, actions_, rewards_, new_obs_, terminateds_):\n",
    "            self.buffer.append((o, a, r, no, t))\n",
    "        if len(self.buffer) > self.max_size:\n",
    "            self.buffer = self.buffer[-self.max_size:]\n",
    "\n",
    "    def sample_minibatch(self, batch_size):\n",
    "        r = random.sample(self.buffer, min(len(self.buffer), batch_size))\n",
    "        return r\n",
    "    \n",
    "    def load_from_file(self, path):\n",
    "        with open(path, 'rb') as f:\n",
    "            lst = pickle.load(f)\n",
    "        self.buffer += lst\n",
    "        if len(self.buffer) > self.max_size:\n",
    "            self.buffer = self.buffer[-self.max_size:]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average return since last print: -113.01152249009446 in step 10\n",
      "average return since last print: -191.8076237943239 in step 20\n",
      "average return since last print: -147.18086310215585 in step 30\n",
      "average return since last print: -69.145859312376 in step 40\n",
      "average return since last print: -149.24887289531358 in step 50\n",
      "average return since last print: -161.71989557964716 in step 60\n",
      "average return since last print: -122.84228365862855 in step 70\n",
      "average return since last print: -123.73784293075593 in step 80\n",
      "average return since last print: -184.92750585255058 in step 90\n",
      "average return since last print: -106.50915910947009 in step 100\n",
      "average return since last print: -115.1366378890804 in step 110\n",
      "average return since last print: -215.10463943725333 in step 120\n",
      "average return since last print: -244.99161285254715 in step 130\n",
      "average return since last print: -179.30571684388218 in step 140\n",
      "average return since last print: -247.72557568663305 in step 150\n",
      "average return since last print: -222.00261089635626 in step 160\n",
      "average return since last print: -205.10947837256745 in step 170\n",
      "average return since last print: -296.103532115109 in step 180\n",
      "average return since last print: -143.8689423877328 in step 190\n",
      "average return since last print: -115.49462830157117 in step 200\n",
      "average return since last print: -135.54791818585488 in step 210\n",
      "average return since last print: -221.16478958119902 in step 220\n",
      "average return since last print: -195.39657003784393 in step 230\n",
      "average return since last print: -152.47091448863944 in step 240\n",
      "average return since last print: -197.89167525336453 in step 250\n",
      "average return since last print: -96.90350216181724 in step 260\n",
      "average return since last print: -152.46015306138415 in step 270\n",
      "average return since last print: -129.38973359499235 in step 280\n",
      "average return since last print: -127.76414036652643 in step 290\n",
      "average return since last print: -91.51244892316068 in step 300\n",
      "average return since last print: -258.51296846896463 in step 310\n",
      "average return since last print: -114.67589298347785 in step 320\n",
      "average return since last print: -95.50073063926055 in step 330\n",
      "average return since last print: -190.0668756333548 in step 340\n",
      "average return since last print: -280.04439673899674 in step 350\n",
      "average return since last print: -198.8214524168101 in step 360\n",
      "average return since last print: -125.6202560309337 in step 370\n",
      "average return since last print: -281.35174023085324 in step 380\n",
      "average return since last print: -165.53941434633612 in step 390\n",
      "average return since last print: -118.09676148211568 in step 400\n",
      "average return since last print: -174.7570948107799 in step 410\n",
      "average return since last print: -75.99793561449349 in step 420\n",
      "average return since last print: -222.29644037763538 in step 430\n",
      "average return since last print: -76.74376390869647 in step 440\n",
      "average return since last print: -70.01740199430058 in step 450\n",
      "average return since last print: -142.89559482332024 in step 460\n",
      "average return since last print: -169.91656644902537 in step 470\n",
      "average return since last print: -144.5817103005816 in step 480\n",
      "average return since last print: -156.16097387793118 in step 490\n",
      "average return since last print: -171.5710412301723 in step 500\n",
      "average return since last print: -177.02434572760998 in step 510\n",
      "average return since last print: -164.54327757983737 in step 520\n",
      "average return since last print: -146.8620260356303 in step 530\n",
      "average return since last print: -75.27957740042226 in step 540\n",
      "average return since last print: -102.9577480519517 in step 550\n",
      "average return since last print: -206.50955531124697 in step 560\n",
      "average return since last print: -106.83680000326729 in step 570\n",
      "average return since last print: -141.28896752758553 in step 580\n",
      "average return since last print: -208.7192062307151 in step 590\n",
      "average return since last print: -247.27055743926405 in step 600\n",
      "average return since last print: -124.55875243373525 in step 610\n",
      "average return since last print: -99.47304591446476 in step 620\n",
      "average return since last print: -160.69292363689732 in step 630\n",
      "average return since last print: -155.56235756284494 in step 640\n",
      "average return since last print: -160.9884535102163 in step 650\n",
      "average return since last print: -116.47729255610534 in step 660\n",
      "average return since last print: -163.29041055280757 in step 670\n",
      "average return since last print: -111.30471478703433 in step 680\n",
      "average return since last print: -153.77424410799605 in step 690\n",
      "average return since last print: -185.06169603757283 in step 700\n",
      "average return since last print: -89.61799536652482 in step 710\n",
      "average return since last print: -55.370359294228365 in step 720\n",
      "average return since last print: -93.39852437218168 in step 730\n",
      "average return since last print: -82.9275202133001 in step 740\n",
      "average return since last print: -85.16922645821607 in step 750\n",
      "average return since last print: -56.612496311257665 in step 760\n",
      "average return since last print: -64.29422012339161 in step 770\n",
      "average return since last print: -48.023255560650504 in step 780\n",
      "average return since last print: -51.19318493961549 in step 790\n",
      "average return since last print: -25.227347342575158 in step 800\n",
      "average return since last print: -69.20789044880067 in step 810\n",
      "average return since last print: -53.84899548230592 in step 820\n",
      "average return since last print: -52.65591760785802 in step 830\n",
      "average return since last print: -63.82661870134941 in step 840\n",
      "average return since last print: -44.90417108654947 in step 850\n",
      "average return since last print: -32.45131036270478 in step 860\n",
      "average return since last print: -57.38869517090503 in step 870\n",
      "average return since last print: -27.793864322354374 in step 880\n",
      "average return since last print: -29.2630611678513 in step 890\n",
      "average return since last print: -40.283193808803325 in step 900\n",
      "average return since last print: -26.595367132410015 in step 910\n",
      "average return since last print: -36.97408265470244 in step 920\n",
      "average return since last print: -70.44441758450007 in step 930\n",
      "average return since last print: -122.30625741469112 in step 940\n",
      "average return since last print: -32.67261842137616 in step 950\n",
      "average return since last print: -39.35948766935429 in step 960\n",
      "average return since last print: -27.63027247934494 in step 970\n",
      "average return since last print: -54.67716096881416 in step 980\n",
      "average return since last print: -40.432838893390624 in step 990\n",
      "average return since last print: -77.20462489150098 in step 1000\n",
      "average return since last print: -45.917927253790715 in step 1010\n",
      "average return since last print: -81.31710478509578 in step 1020\n",
      "no new returns in step 1030\n",
      "average return since last print: -63.32143980654554 in step 1040\n",
      "average return since last print: -57.67904038637464 in step 1050\n",
      "no new returns in step 1060\n",
      "average return since last print: 32.583308075690866 in step 1070\n",
      "average return since last print: -82.61247952178694 in step 1080\n",
      "no new returns in step 1090\n",
      "average return since last print: 128.44281363817026 in step 1100\n",
      "average return since last print: -35.52632384233839 in step 1110\n",
      "average return since last print: -0.5837447164535234 in step 1120\n",
      "average return since last print: -33.70796729924006 in step 1130\n",
      "average return since last print: -47.04039515154625 in step 1140\n",
      "average return since last print: -45.97678425395991 in step 1150\n",
      "average return since last print: -102.17736744838042 in step 1160\n",
      "average return since last print: -98.747552252944 in step 1170\n",
      "average return since last print: 8.987934010671736 in step 1180\n",
      "average return since last print: -41.463203820487685 in step 1190\n",
      "average return since last print: -45.294061886058856 in step 1200\n",
      "average return since last print: -10.140852153072828 in step 1210\n",
      "average return since last print: -29.295847379106124 in step 1220\n",
      "average return since last print: -35.86818977348901 in step 1230\n",
      "average return since last print: 4.649411697668967 in step 1240\n",
      "average return since last print: -15.116932349241573 in step 1250\n",
      "average return since last print: -80.87140847762969 in step 1260\n",
      "average return since last print: -1.853119370479675 in step 1270\n",
      "average return since last print: -52.80015105265739 in step 1280\n",
      "average return since last print: -79.57422023475598 in step 1290\n",
      "average return since last print: 13.797747034266337 in step 1300\n",
      "average return since last print: -60.976470706253 in step 1310\n",
      "average return since last print: -24.78773022177419 in step 1320\n",
      "average return since last print: -25.383180357907577 in step 1330\n",
      "average return since last print: -3.9960392011990606 in step 1340\n",
      "average return since last print: -22.651946339947287 in step 1350\n",
      "average return since last print: -180.6848108112938 in step 1360\n",
      "average return since last print: -55.929309156288 in step 1370\n",
      "average return since last print: 29.862227138751564 in step 1380\n",
      "no new returns in step 1390\n",
      "average return since last print: 51.4515684897971 in step 1400\n",
      "average return since last print: -71.8116156412773 in step 1410\n",
      "no new returns in step 1420\n",
      "average return since last print: -48.52772100502849 in step 1430\n",
      "average return since last print: 23.221869210716036 in step 1440\n",
      "average return since last print: -46.013524260459775 in step 1450\n",
      "average return since last print: -6.202266976671421 in step 1460\n",
      "average return since last print: -11.427831543819437 in step 1470\n",
      "average return since last print: -17.164497405161853 in step 1480\n",
      "no new returns in step 1490\n",
      "no new returns in step 1500\n",
      "no new returns in step 1510\n",
      "no new returns in step 1520\n",
      "average return since last print: 5.715028128662141 in step 1530\n",
      "average return since last print: 22.071053358641137 in step 1540\n",
      "average return since last print: 80.73472499789438 in step 1550\n",
      "no new returns in step 1560\n",
      "no new returns in step 1570\n",
      "no new returns in step 1580\n",
      "average return since last print: 95.47072671602581 in step 1590\n",
      "no new returns in step 1600\n",
      "average return since last print: -96.3633762210962 in step 1610\n",
      "no new returns in step 1620\n",
      "no new returns in step 1630\n",
      "average return since last print: -59.27960827529265 in step 1640\n",
      "no new returns in step 1650\n",
      "average return since last print: 72.29045301079941 in step 1660\n",
      "average return since last print: 27.382369989365507 in step 1670\n",
      "no new returns in step 1680\n",
      "average return since last print: -10.029706155414559 in step 1690\n",
      "no new returns in step 1700\n",
      "no new returns in step 1710\n",
      "no new returns in step 1720\n",
      "no new returns in step 1730\n",
      "no new returns in step 1740\n",
      "no new returns in step 1750\n",
      "no new returns in step 1760\n",
      "no new returns in step 1770\n",
      "average return since last print: 72.53921699548508 in step 1780\n",
      "no new returns in step 1790\n",
      "average return since last print: 54.14222033839047 in step 1800\n",
      "no new returns in step 1810\n",
      "no new returns in step 1820\n",
      "no new returns in step 1830\n",
      "no new returns in step 1840\n",
      "no new returns in step 1850\n",
      "no new returns in step 1860\n",
      "no new returns in step 1870\n",
      "no new returns in step 1880\n",
      "no new returns in step 1890\n",
      "no new returns in step 1900\n",
      "average return since last print: 19.822918623947146 in step 1910\n",
      "average return since last print: 153.16952131776608 in step 1920\n",
      "average return since last print: -32.49844410666307 in step 1930\n",
      "average return since last print: 4.915993636185064 in step 1940\n",
      "average return since last print: 12.600044230572095 in step 1950\n",
      "no new returns in step 1960\n",
      "average return since last print: 21.659683951552978 in step 1970\n",
      "average return since last print: 26.82736026510078 in step 1980\n",
      "average return since last print: -20.81022887777165 in step 1990\n",
      "average return since last print: 41.20292322676312 in step 2000\n"
     ]
    }
   ],
   "source": [
    "#init environments\n",
    "envs = Environment(NUM_ENVS)\n",
    "\n",
    "#just some approximatory metrics\n",
    "returns = np.zeros(NUM_ENVS)\n",
    "return_tracker = []\n",
    "\n",
    "# Create the model\n",
    "Q_theta = create_model(regularization_factor)\n",
    "Q_target = tf.keras.models.clone_model(Q_theta)\n",
    "\n",
    "# init buffer\n",
    "kabuff = Buffer(BUFFER_SIZE)\n",
    "# prefill buffer\n",
    "if use_prefill:\n",
    "    kabuff.load_from_file(\"kabuff.pkl\") #kabuff.pkl contains samples derived from a previous working policy\n",
    "\n",
    "STEPS = 0\n",
    "\n",
    "while not CONVERGED and STEPS < MAX_STEPS:\n",
    "    STEPS += 1\n",
    "    \n",
    "    Q_target.set_weights(Q_theta.get_weights()) #update target network to current training network interation\n",
    "    epsilon = max(minimum_epsilon, epsilon*epsilon_decay) #anneal the epsilon used for sampling\n",
    "    \n",
    "    for n in range(N):\n",
    "        \n",
    "        new_samples = envs.sample(Q_theta, epsilon) #sample from the environment\n",
    "        \n",
    "        #update return-metrics, reset returns if terminated\n",
    "        #envs.sample returns a tuple of NUM_ENVS long np.arrays corresponding to old_obs, actions, rewards, new_obs, terminated\n",
    "        #we add the rewards to our return counters\n",
    "        returns += new_samples[2] #new_samples[2] = rewards\n",
    "        for i, t in enumerate(new_samples[4]): #new_samples[4] = terminateds\n",
    "            if t:\n",
    "                #environment terminated, add to tracker and reset\n",
    "                return_tracker.append(returns[i])\n",
    "                returns[i] = 0\n",
    "\n",
    "        kabuff.add_to_buffer(new_samples) #add environment samples to the buffer\n",
    "\n",
    "        for k in range(K):\n",
    "            #sample s,a,r,s' minibatch from buffer\n",
    "            minibatch = kabuff.sample_minibatch(MINI_BATCH_SIZE)\n",
    "\n",
    "            #unpack the minibatch\n",
    "            new_states = np.array([sample[3] for sample in minibatch])\n",
    "            rewards = np.array([sample[2] for sample in minibatch])\n",
    "            actions = np.array([sample[1] for sample in minibatch])\n",
    "            old_states = np.array([sample[0] for sample in minibatch])\n",
    "            terminateds = np.array([sample[4] for sample in minibatch])\n",
    "\n",
    "\n",
    "            #DDQN\n",
    "            if use_DDQN:\n",
    "                Q_target_values = Q_target(new_states)\n",
    "                Q_theta_values = Q_theta(new_states)\n",
    "                max_Q_target_values = []\n",
    "                for target_action_vals, theta_action_vals in zip(Q_target_values, Q_theta_values):\n",
    "                    Q_theta_max_index = np.argmax(theta_action_vals)\n",
    "                    max_Q_target_value = target_action_vals[Q_theta_max_index]\n",
    "                    max_Q_target_values.append(max_Q_target_value)\n",
    "                max_Q_target_values = np.array(max_Q_target_values)\n",
    "            else:\n",
    "                Q_target_values = Q_target(new_states)\n",
    "                max_Q_target_values = np.array([max(action_values) for action_values in Q_target_values])\n",
    "\n",
    "\n",
    "            #calculate the targets, don't add new Q_target_values if an environment just terminated\n",
    "            target_q_values = rewards + GAMMA * max_Q_target_values * (1-terminateds)\n",
    "\n",
    "            #training: do gradient descent of Q_theta in direction of target\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = Q_theta(old_states)\n",
    "                selected_q_values = tf.gather(predictions, actions, batch_dims=1) #predictions of Q(old_states, actions)\n",
    "                loss_value = tf.reduce_mean(tf.square(selected_q_values-target_q_values)) #compute simple MSE loss\n",
    "            gradients = tape.gradient(loss_value, Q_theta.trainable_variables) #get\n",
    "            optimizer.apply_gradients(zip(gradients, Q_theta.trainable_variables)) #and apply gradients\n",
    "\n",
    "    #every 10 steps, print log message and reset returns\n",
    "    if STEPS % 10 == 0:\n",
    "        if(return_tracker):\n",
    "            print(\"average return since last print: \" + str(np.mean(return_tracker)) + \" in step \" + str(STEPS))\n",
    "            return_tracker = []\n",
    "        else:\n",
    "            print(\"no new returns in step \" + str(STEPS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: seventh_try/assets\n"
     ]
    }
   ],
   "source": [
    "Q_theta.save('final_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-15 09:33:42.560151: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "#Q_theta = tf.keras.models.load_model(\"fourth_try\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = gym.make('LunarLander-v2', render_mode='human')\n",
    "#test_env = gym.make('CartPole-v1', render_mode='human')\n",
    "obs, inf = test_env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    qs = Q_theta(tf.expand_dims(obs, 0))\n",
    "    act = np.argmax(qs)\n",
    "    obs, _, terminated, _, _ = test_env.step(act)\n",
    "    if(terminated):\n",
    "        obs, _ = test_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
